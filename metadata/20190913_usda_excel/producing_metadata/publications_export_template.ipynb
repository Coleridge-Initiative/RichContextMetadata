{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating `publications.json` partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a template notebook for generating metadata on publications - most importantly, the linkage between the publication and dataset (datasets are enumerated in `datasets.json`)\n",
    "\n",
    "Process goes as follows:\n",
    "1. Import CSV with publication-dataset linkages. Format and clean\n",
    "2. Match to `datasets.json` -- alert if given dataset doesn't exist yet\n",
    "3. Generate list of dicts with publication metadata\n",
    "4. Write to a `publications.json` file\n",
    "\n",
    "Questions? Email or slack Sophie at sr2661@nyu.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import CSV containing publication-dataset linkages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set `linkages_path` to the location of the csv containg dataset-publication linkages and read in csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "linkages_path =  '/Users/sophierand/RichContextMetadata/metadata/20190913_usda_excel/producing_metadata/usda_linkages.csv'\n",
    "linkages_csv = pd.read_csv(linkages_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Format/clean linkage data\n",
    "1. Update `pub_metadata_fields` with the fields that exist in your linkage file, eg: \n",
    "    * `title` (required)\n",
    "    * `dataset_id` (required)\n",
    "    * `url` OR `doi` (at least one is required for every linkage)\n",
    "    * `journal`\n",
    "\n",
    "2. Remove possible null values in the `dataset_id` field (update field name to match your csv if needed)\n",
    "3. Deduplicate\n",
    "4. Apply `scrub_unicode` to `title` field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrub_unicode (text):\n",
    "    \"\"\"\n",
    "    try to handle the unicode edge cases encountered in source text,\n",
    "    as best as possible\n",
    "    \"\"\"\n",
    "    x = \" \".join(map(lambda s: s.strip(), text.split(\"\\n\"))).strip()\n",
    "\n",
    "    x = x.replace('“', '\"').replace('”', '\"')\n",
    "    x = x.replace(\"‘\", \"'\").replace(\"’\", \"'\").replace(\"`\", \"'\")\n",
    "    x = x.replace(\"`` \", '\"').replace(\"''\", '\"')\n",
    "    x = x.replace('…', '...').replace(\"\\\\u2026\", \"...\")\n",
    "    x = x.replace(\"\\\\u00ae\", \"\").replace(\"\\\\u2122\", \"\")\n",
    "    x = x.replace(\"\\\\u00a0\", \" \").replace(\"\\\\u2022\", \"*\").replace(\"\\\\u00b7\", \"*\")\n",
    "    x = x.replace(\"\\\\u2018\", \"'\").replace(\"\\\\u2019\", \"'\").replace(\"\\\\u201a\", \"'\")\n",
    "    x = x.replace(\"\\\\u201c\", '\"').replace(\"\\\\u201d\", '\"')\n",
    "\n",
    "    x = x.replace(\"\\\\u20ac\", \"€\")\n",
    "    x = x.replace(\"\\\\u2212\", \" - \") # minus sign\n",
    "\n",
    "    x = x.replace(\"\\\\u00e9\", \"é\")\n",
    "    x = x.replace(\"\\\\u017c\", \"ż\").replace(\"\\\\u015b\", \"ś\").replace(\"\\\\u0142\", \"ł\")    \n",
    "    x = x.replace(\"\\\\u0105\", \"ą\").replace(\"\\\\u0119\", \"ę\").replace(\"\\\\u017a\", \"ź\").replace(\"\\\\u00f3\", \"ó\")\n",
    "\n",
    "    x = x.replace(\"\\\\u2014\", \" - \").replace('–', '-').replace('—', ' - ')\n",
    "    x = x.replace(\"\\\\u2013\", \" - \").replace(\"\\\\u00ad\", \" - \")\n",
    "\n",
    "    x = str(unicodedata.normalize(\"NFKD\", x).encode(\"ascii\", \"ignore\").decode(\"utf-8\"))\n",
    "\n",
    "    # some content returns text in bytes rather than as a str ?\n",
    "    try:\n",
    "        assert type(x).__name__ == \"str\"\n",
    "    except AssertionError:\n",
    "        print(\"not a string?\", type(x), x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pub_metadata_fields = ['title','pub_url','dataset_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "linkages_csv = linkages_csv[pub_metadata_fields]\n",
    "linkages_csv = linkages_csv.loc[pd.notnull(linkages_csv.dataset_id)].drop_duplicates()\n",
    "linkages_csv['title'] = linkages_csv['title'].apply(scrub_unicode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate list of dicts of metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in `datasets.json`. Update `datasets_path` to your local."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_path = '/Users/sophierand/RCDatasets/datasets.json'\n",
    "\n",
    "with open(datasets_path) as json_file:\n",
    "    datasets = json.load(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create list of dictionaries of publication metadata. `format_metadata` iterrates through `linkages_csv` dataframe, splits the `dataset_id` field (for when multiple datasets are listed); throws an error if the dataset doesn't exist and needs to be added to `datasets.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_linkages(linkages_csv,datasets):\n",
    "    linkage_list = []\n",
    "    for i,r in linkages_csv.iterrows():\n",
    "        ds_id_list = [d.strip() for d in r['dataset_id'].split(\",\")]\n",
    "        ds_id_dict_list = [{'dataset_id':d} for d in ds_id_list]\n",
    "        pub_dict = {'title':r['title'],'url':r['pub_url'],'related_dataset':ds_id_dict_list}\n",
    "        for ds in ds_id_list:\n",
    "            check_ds = [b for b in datasets if b['id'] == ds]\n",
    "            if len(check_ds) == 0:\n",
    "                print('dataset {} isnt listed in datasets.json. Please add to file'.format(ds))\n",
    "        linkage_list.append(pub_dict)\n",
    "    return linkage_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate publication metadata and export to json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "linkage_list = format_linkages(linkages_csv,datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update `json_pub_path` to be: \n",
    "`<name_of_subfolder>_publications.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "json_pub_path = '20190920_excel_usda_publications.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(json_pub_path, 'w') as outfile:\n",
    "    json.dump(linkage_list, outfile, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
