{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating `publications.json` partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a template notebook for generating metadata on publications - most importantly, the linkage between the publication and dataset (datasets are enumerated in `datasets.json`)\n",
    "\n",
    "Process goes as follows:\n",
    "1. Import CSV with publication-dataset linkages. Your csv should have at the minimum, fields (spelled like the below):\n",
    "    * `dataset` to hold the dataset_ids, and \n",
    "    * `title` for the publication title. \n",
    "\n",
    "Update the csv with these field names to ensure this code will run.  We read in, dedupe and format the title\n",
    "2. Match to `datasets.json` -- alert if given dataset doesn't exist yet\n",
    "3. Generate list of dicts with publication metadata\n",
    "4. Write to a publications.json file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import CSV containing publication-dataset linkages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set `linkages_path` to the location of the csv containg dataset-publication linkages and read in csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'foodaps_usda_cleaned.csv'\n",
    "rcm_subfolder = '20190619_usda_foodaps'\n",
    "\n",
    "linkages_path =  os.path.join('/Users/andrewnorris/RichContextMetadata/metadata',rcm_subfolder,file_name)\n",
    "# linkages_path =  os.path.join(os.getcwd(),'SNAP_DATA_DIMENSIONS_SEARCH_DEMO.csv')\n",
    "linkages_csv = pd.read_csv(linkages_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/andrewnorris/RichContextMetadata/metadata/20190619_usda_foodaps/foodaps_usda_cleaned.csv'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linkages_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Format/clean linkage data - apply `scrub_unicode` to `title` field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrub_unicode (text):\n",
    "    \"\"\"\n",
    "    try to handle the unicode edge cases encountered in source text,\n",
    "    as best as possible\n",
    "    \"\"\"\n",
    "    x = \" \".join(map(lambda s: s.strip(), text.split(\"\\n\"))).strip()\n",
    "\n",
    "    x = x.replace('“', '\"').replace('”', '\"')\n",
    "    x = x.replace(\"‘\", \"'\").replace(\"’\", \"'\").replace(\"`\", \"'\")\n",
    "    x = x.replace(\"`` \", '\"').replace(\"''\", '\"')\n",
    "    x = x.replace('…', '...').replace(\"\\\\u2026\", \"...\")\n",
    "    x = x.replace(\"\\\\u00ae\", \"\").replace(\"\\\\u2122\", \"\")\n",
    "    x = x.replace(\"\\\\u00a0\", \" \").replace(\"\\\\u2022\", \"*\").replace(\"\\\\u00b7\", \"*\")\n",
    "    x = x.replace(\"\\\\u2018\", \"'\").replace(\"\\\\u2019\", \"'\").replace(\"\\\\u201a\", \"'\")\n",
    "    x = x.replace(\"\\\\u201c\", '\"').replace(\"\\\\u201d\", '\"')\n",
    "\n",
    "    x = x.replace(\"\\\\u20ac\", \"€\")\n",
    "    x = x.replace(\"\\\\u2212\", \" - \") # minus sign\n",
    "\n",
    "    x = x.replace(\"\\\\u00e9\", \"é\")\n",
    "    x = x.replace(\"\\\\u017c\", \"ż\").replace(\"\\\\u015b\", \"ś\").replace(\"\\\\u0142\", \"ł\")    \n",
    "    x = x.replace(\"\\\\u0105\", \"ą\").replace(\"\\\\u0119\", \"ę\").replace(\"\\\\u017a\", \"ź\").replace(\"\\\\u00f3\", \"ó\")\n",
    "\n",
    "    x = x.replace(\"\\\\u2014\", \" - \").replace('–', '-').replace('—', ' - ')\n",
    "    x = x.replace(\"\\\\u2013\", \" - \").replace(\"\\\\u00ad\", \" - \")\n",
    "\n",
    "    x = str(unicodedata.normalize(\"NFKD\", x).encode(\"ascii\", \"ignore\").decode(\"utf-8\"))\n",
    "\n",
    "    # some content returns text in bytes rather than as a str ?\n",
    "    try:\n",
    "        assert type(x).__name__ == \"str\"\n",
    "    except AssertionError:\n",
    "        print(\"not a string?\", type(x), x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrub titles of problematic characters, drop nulls and dedupe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>doi</th>\n",
       "      <th>journal</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dataset-025</td>\n",
       "      <td>http://www.ers.usda.gov/publications/pub-detai...</td>\n",
       "      <td>United States Department of Agriculture, Econo...</td>\n",
       "      <td>WIC Households' Food Purchases Using WIC Benef...</td>\n",
       "      <td>http://www.ers.usda.gov/publications/pub-detai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dataset-025</td>\n",
       "      <td>https://www.ers.usda.gov/publications/pub-deta...</td>\n",
       "      <td>United States Department of Agriculture, Econo...</td>\n",
       "      <td>USDA's National Household Food Acquisition and...</td>\n",
       "      <td>https://ageconsearch.umn.edu/record/276252/fil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dataset-025</td>\n",
       "      <td>10.1016/j.foodpol.2018.03.007</td>\n",
       "      <td>Food Policy</td>\n",
       "      <td>Overview: Exploring ways to encourage healthie...</td>\n",
       "      <td>https://api.elsevier.com/content/article/PII:S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dataset-025</td>\n",
       "      <td>https://www.ers.usda.gov/amber-waves/2018/janu...</td>\n",
       "      <td>United States Department of Agriculture, Econo...</td>\n",
       "      <td>Supermarkets, Schools, and Social Gatherings: ...</td>\n",
       "      <td>https://www.ers.usda.gov/amber-waves/2018/janu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dataset-025</td>\n",
       "      <td>https://www.ers.usda.gov/publications/pub-deta...</td>\n",
       "      <td>United States Department of Agriculture, Econo...</td>\n",
       "      <td>Nutritional Quality of Foods Acquired by Ameri...</td>\n",
       "      <td>https://www.ers.usda.gov/webdocs/publications/...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       dataset                                                doi  \\\n",
       "0  dataset-025  http://www.ers.usda.gov/publications/pub-detai...   \n",
       "1  dataset-025  https://www.ers.usda.gov/publications/pub-deta...   \n",
       "2  dataset-025                      10.1016/j.foodpol.2018.03.007   \n",
       "3  dataset-025  https://www.ers.usda.gov/amber-waves/2018/janu...   \n",
       "4  dataset-025  https://www.ers.usda.gov/publications/pub-deta...   \n",
       "\n",
       "                                             journal  \\\n",
       "0  United States Department of Agriculture, Econo...   \n",
       "1  United States Department of Agriculture, Econo...   \n",
       "2                                        Food Policy   \n",
       "3  United States Department of Agriculture, Econo...   \n",
       "4  United States Department of Agriculture, Econo...   \n",
       "\n",
       "                                               title  \\\n",
       "0  WIC Households' Food Purchases Using WIC Benef...   \n",
       "1  USDA's National Household Food Acquisition and...   \n",
       "2  Overview: Exploring ways to encourage healthie...   \n",
       "3  Supermarkets, Schools, and Social Gatherings: ...   \n",
       "4  Nutritional Quality of Foods Acquired by Ameri...   \n",
       "\n",
       "                                                 url  \n",
       "0  http://www.ers.usda.gov/publications/pub-detai...  \n",
       "1  https://ageconsearch.umn.edu/record/276252/fil...  \n",
       "2  https://api.elsevier.com/content/article/PII:S...  \n",
       "3  https://www.ers.usda.gov/amber-waves/2018/janu...  \n",
       "4  https://www.ers.usda.gov/webdocs/publications/...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linkages_csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "normalize() argument 2 must be unicode, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-41e5abad83b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlinkages_csv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'title'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinkages_csv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'title'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscrub_unicode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlinkages_csv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinkages_csv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinkages_csv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_duplicates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlinkages_csv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinkages_csv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinkages_csv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_duplicates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/andrewnorris/anaconda2/lib/python2.7/site-packages/pandas/core/series.pyc\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   3589\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3590\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3591\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3593\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-5d4d0ecc0c65>\u001b[0m in \u001b[0;36mscrub_unicode\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\\\u2013\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" - \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\\\u00ad\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" - \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0municodedata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"NFKD\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ascii\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;31m# some content returns text in bytes rather than as a str ?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: normalize() argument 2 must be unicode, not str"
     ]
    }
   ],
   "source": [
    "linkages_csv['title'] = linkages_csv['title'].apply(scrub_unicode)\n",
    "linkages_csv = linkages_csv.loc[pd.notnull(linkages_csv.dataset)].drop_duplicates()\n",
    "linkages_csv = linkages_csv.loc[pd.notnull(linkages_csv.title)].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pub_metadata_fields = ['title']\n",
    "original_metadata_cols = list(set(linkages_csv.columns.values.tolist()) - set(pub_metadata_fields)-set(['dataset']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate list of dicts of metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in `datasets.json`. Update `datasets_path` to your local."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_path = '/Users/sophierand/RCDatasets/datasets.json'\n",
    "\n",
    "with open(datasets_path) as json_file:\n",
    "    datasets = json.load(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create list of dictionaries of publication metadata. `format_metadata` iterrates through `linkages_csv` dataframe, splits the `dataset` field (for when multiple datasets are listed); throws an error if the dataset doesn't exist and needs to be added to `datasets.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pub_dict(linkages_dataframe,datasets):\n",
    "    pub_dict_list = []\n",
    "    for i, r in linkages_dataframe.iterrows():\n",
    "        r['title'] = scrub_unicode(r['title'])\n",
    "        ds_id_list = [f for f in [d.strip() for d in r['dataset'].split(\",\")] if f not in [\"\",\" \"]]\n",
    "        for ds in ds_id_list:\n",
    "            check_ds = [b for b in datasets if b['id'] == ds]\n",
    "            if len(check_ds) == 0:\n",
    "                print('dataset {} isnt listed in datasets.json. Please add to file'.format(ds))\n",
    "        required_metadata = r[pub_metadata_fields].to_dict()\n",
    "        required_metadata.update({'datasets':ds_id_list})\n",
    "        pub_dict = required_metadata\n",
    "        if len(original_metadata_cols) > 0:\n",
    "            original_metadata = r[original_metadata_cols].to_dict()\n",
    "            original_metadata.update({'date_added':datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')})\n",
    "            pub_dict.update({'original':original_metadata})\n",
    "        pub_dict_list.append(pub_dict)\n",
    "    return pub_dict_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate publication metadata and export to json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "linkage_list = create_pub_dict(linkages_csv,datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update `pub_path` to be: \n",
    "`<name_of_subfolder>_publications.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_pub_path = os.path.join('/Users/sophierand/RCPublications/partitions/',rcm_subfolder+'_publications.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(json_pub_path, 'w') as outfile:\n",
    "    json.dump(linkage_list, outfile, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
