{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook Untitled2.ipynb to script\n",
      "[NbConvertApp] Writing 4227 bytes to Untitled2.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to script Untitled2.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import datetime\n",
    "import os\n",
    "import json\n",
    "from os import listdir\n",
    "import pandas as pd\n",
    "pd.set_option('display.width', 10000)\n",
    "import dateutil.parser\n",
    "import metadata_funs\n",
    "import xlrd\n",
    "import datetime\n",
    "from os.path import isfile, join\n",
    "import ntpath\n",
    "import uuid\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_dictionary():\n",
    "    ex_data_path = os.path.join(os.getcwd(),'metadata/manually_curated_metadata/ADRF Dataset Metadata.xlsx')\n",
    "    xls = pd.ExcelFile(ex_data_path)\n",
    "    sheet_to_df_map = pd.read_excel(ex_data_path, sheet_name=None)\n",
    "    uniform_sheets = ['course1-datasets','course2-datasets' ,'kcmo-datasets', 'in_data_2019-datasets'\n",
    "    , 'mo_data_2019-datasets', 'usda-datasets','bundesbank-rc']\n",
    "    lim_excel_df_sheets  = []\n",
    "    for i,v in enumerate(sheet_to_df_map):\n",
    "        if v in uniform_sheets:\n",
    "            lim_excel_df_sheets.append(sheet_to_df_map[v])\n",
    "    common_fields = list(set(lim_excel_df_sheets[6].columns.values) & set(lim_excel_df_sheets[5].columns.values))\n",
    "    new_df_list = []\n",
    "    for i in range(len(lim_excel_df_sheets)):\n",
    "        a = lim_excel_df_sheets[i][common_fields]\n",
    "        new_df_list.append(a)\n",
    "    df = pd.concat(new_df_list).drop_duplicates()\n",
    "    pattern = re.compile('([^\\sa-zA-Z]|_)+')\n",
    "    df['title'] = df['title'].apply(lambda x: pattern.sub('',x).strip())\n",
    "    df['title'] = df['title'].apply(lambda x: re.sub(' +', ' ',x))\n",
    "    return df  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_curated_linkages():\n",
    "    linkage_path = os.path.join(os.getcwd(),'metadata/manually_curated_metadata/curated_linkages.csv')\n",
    "    csv = pd.read_csv(linkage_path)\n",
    "    curated_dataset_names = csv.dataset_name.unique().tolist()\n",
    "    return curated_dataset_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_manual_ds_names():\n",
    "    manual_dataset_json_path =  os.path.join(os.getcwd(),'metadata/manually_curated_metadata/curated_dataset_names.json')\n",
    "    with open(manual_dataset_json_path) as json_file:\n",
    "        manual_dataset_json = json.load(json_file)\n",
    "    return manual_dataset_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_manual_data():\n",
    "    manual_dataset_names_list = read_manual_ds_names()\n",
    "    manual_dataset_names = [d['dataset_name'] for d in manual_dataset_names_list]\n",
    "    curated_dataset_names = read_curated_linkages()\n",
    "    dataset_names_list = list(set(manual_dataset_names + curated_dataset_names))\n",
    "    return dataset_names_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_dataset_dfs(dataset_names_list,dataset_df):\n",
    "    addl_datasets = [c for c in dataset_names_list if c not in dataset_df.title.unique().tolist()]\n",
    "    tmp = pd.DataFrame({'description': '','data_steward_org': '',  'temporal_coverage_end': '',  'source_archive': '',\n",
    "  'dataset_documentation': '',  'data_classification': '',  'external_id': '',  'keywords': '',  'access_actions_required': '',\n",
    "  'temporal_coverage_start': '',  'dataset_version': '',\n",
    "  'access_requirements': '',  'dataset_header_desc': '',  'dataset_citation': '',  'title': '',  'category': '',\n",
    "  'geographical_coverage': '','data_steward': '',  'reference_url': '',  'source_url': '',  'related_articles': '',\n",
    "  'adrf_id': '',  'geographical_unit': '',  'data_usage_policy': '',  'data_provider': '','filenames': '','title':addl_datasets})\n",
    "    data_df_full = pd.concat([dataset_df, tmp])\n",
    "    return data_df_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sophierand/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:9: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "dataset_names_list = combine_manual_data()\n",
    "dataset_df  = read_data_dictionary()\n",
    "data_df_full = concat_dataset_dfs(dataset_names_list = dataset_names_list,dataset_df = dataset_df)\n",
    "data_df_full['dataset_id'] = data_df_full.title.apply(lambda x: \"dataset-{}\".format(metadata_funs.get_hash(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df_full = data_df_full.drop_duplicates()\n",
    "dd_dict = data_df_full.to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in dd_dict:\n",
    "    i['title'] = unicodedata.normalize('NFC',i['title'])\n",
    "    if isinstance(i['temporal_coverage_end'], datetime.date):\n",
    "        i['temporal_coverage_end'] = str(dateutil.parser.parse(str(i['temporal_coverage_end'])).date())\n",
    "    if isinstance(i['temporal_coverage_start'], datetime.date):\n",
    "        i['temporal_coverage_start'] = str(dateutil.parser.parse(str(i['temporal_coverage_start'])).date())\n",
    "\n",
    "\n",
    "json.dump(dd_dict, open('metadata/datasets.json', 'w'), indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
